{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mybenkhadda/docxchain/blob/main/docxchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JEi_TqGvyFq",
        "outputId": "6403d8df-b11c-4873-f20a-1b8d48b4ec40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'docxchain'...\n",
            "remote: Enumerating objects: 406, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 406 (delta 8), reused 7 (delta 0), pack-reused 364\u001b[K\n",
            "Receiving objects: 100% (406/406), 253.28 MiB | 14.46 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n",
            "Updating files: 100% (342/342), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mybenkhadda/docxchain.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9BsbGJlwPy7",
        "outputId": "1d87252a-da5d-49e2-f6a7-c1c0abeefaff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/docxchain\n"
          ]
        }
      ],
      "source": [
        "%cd docxchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1ScrFUNwmqf",
        "outputId": "56b1e2c1-c1ca-446c-8848-9eb0bddef5c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.1/443.1 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q pdf2image ipdb modelscope datasets==2.18.0 rapid_latex_ocr pyclipper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x4U_D5bjzAw6",
        "outputId": "7d612db4-b562-4de3-d5bc-89a003e76119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow<=2.10\n",
            "  Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow<=2.10)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (1.62.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (3.9.0)\n",
            "Collecting keras<2.11,>=2.10.0 (from tensorflow<=2.10)\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow<=2.10)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (24.0)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow<=2.10)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (1.16.0)\n",
            "Collecting tensorboard<2.11,>=2.10 (from tensorflow<=2.10)\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (0.36.0)\n",
            "Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow<=2.10)\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<=2.10) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<=2.10) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow<=2.10)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow<=2.10)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.11,>=2.10->tensorflow<=2.10)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<=2.10) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<=2.10) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<=2.10) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<=2.10) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<=2.10) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<=2.10) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<=2.10) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<=2.10) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<=2.10) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<=2.10) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<=2.10) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<=2.10) (3.2.2)\n",
            "Installing collected packages: tensorboard-plugin-wit, keras, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-0.4.6 keras-2.10.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "8209c42365bf41f494bdcaf11ceab09c",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --upgrade \"tensorflow<=2.10\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcbbR_EB0giD",
        "outputId": "0fd84ee3-4ea3-4a3e-b6c6-229f530adbfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eunPlRGp91a",
        "outputId": "024b4bf3-3966-4052-bc1b-d62b9a663624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.0/116.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet langchain_experimental langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Gnhh2vEUtgj0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import datetime\n",
        "import random\n",
        "import time\n",
        "import pytz\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pipelines.document_structurization import DocumentStructurization\n",
        "from utilities.visualization import *\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1HhZmKTXtgj1"
      },
      "outputs": [],
      "source": [
        "configs = dict()\n",
        "\n",
        "layout_analysis_configs = dict()\n",
        "layout_analysis_configs['from_modelscope_flag'] = False\n",
        "layout_analysis_configs['model_path'] = './home/DocXLayout_231012.pth'  # note that: currently the layout analysis model is NOT from modelscope\n",
        "configs['layout_analysis_configs'] = layout_analysis_configs\n",
        "\n",
        "text_detection_configs = dict()\n",
        "text_detection_configs['from_modelscope_flag'] = True\n",
        "text_detection_configs['model_path'] = 'damo/cv_resnet18_ocr-detection-line-level_damo'\n",
        "configs['text_detection_configs'] = text_detection_configs\n",
        "\n",
        "text_recognition_configs = dict()\n",
        "text_recognition_configs['from_modelscope_flag'] = True\n",
        "text_recognition_configs['model_path'] = 'damo/cv_convnextTiny_ocr-recognition-document_damo'  # alternatives: 'damo/cv_convnextTiny_ocr-recognition-scene_damo', 'damo/cv_convnextTiny_ocr-recognition-general_damo', 'damo/cv_convnextTiny_ocr-recognition-handwritten_damo'\n",
        "configs['text_recognition_configs'] = text_recognition_configs\n",
        "\n",
        "formula_recognition_configs = dict()\n",
        "formula_recognition_configs['from_modelscope_flag'] = False\n",
        "formula_recognition_configs['image_resizer_path'] = '/home/LaTeX-OCR_image_resizer.onnx'\n",
        "formula_recognition_configs['encoder_path'] = '/home/LaTeX-OCR_encoder.onnx'\n",
        "formula_recognition_configs['decoder_path'] = '/home/LaTeX-OCR_decoder.onnx'\n",
        "formula_recognition_configs['tokenizer_json'] = '/home/LaTeX-OCR_tokenizer.json'\n",
        "configs['formula_recognition_configs'] = formula_recognition_configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4D_4Yl6wn3Av"
      },
      "outputs": [],
      "source": [
        "random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5G74gldtgj1",
        "outputId": "a4762b6f-ce4a-420e-f323-640a61e1ec5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fix size testing.\n",
            "training chunk_sizes: [32]\n",
            "The output will be saved to  /content/docxchain/../../exp/ctdet_subfield/default\n",
            "heads {'hm': 11, 'cls': 4, 'ftype': 3, 'wh': 8, 'hm_sub': 2, 'wh_sub': 8, 'reg': 2, 'reg_sub': 2}\n",
            "[0]\n",
            "--> loading model from local file: ./home/DocXLayout_231012.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-26 07:57:49,391 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
            "Downloading: 100%|██████████| 312M/312M [00:02<00:00, 124MB/s]\n",
            "Downloading: 100%|██████████| 41.7k/41.7k [00:00<00:00, 1.21MB/s]\n",
            "Downloading: 100%|██████████| 8.06M/8.06M [00:00<00:00, 50.3MB/s]\n",
            "Downloading: 0.00B [00:00, ?B/s]\n",
            "Downloading: 100%|██████████| 118/118 [00:00<00:00, 85.2kB/s]\n",
            "Downloading: 100%|██████████| 61.7k/61.7k [00:00<00:00, 1.77MB/s]\n",
            "Downloading: 100%|██████████| 313k/313k [00:00<00:00, 3.26MB/s]\n",
            "Downloading: 100%|██████████| 436k/436k [00:00<00:00, 3.64MB/s]\n",
            "Downloading: 100%|██████████| 24.0/24.0 [00:00<00:00, 18.3kB/s]\n",
            "Downloading: 100%|██████████| 3.93k/3.93k [00:00<00:00, 1.24MB/s]\n",
            "2024-04-26 07:58:05,128 - modelscope - INFO - initiate model from /root/.cache/modelscope/hub/damo/cv_resnet18_ocr-detection-line-level_damo\n",
            "2024-04-26 07:58:05,130 - modelscope - INFO - initiate model from location /root/.cache/modelscope/hub/damo/cv_resnet18_ocr-detection-line-level_damo.\n",
            "2024-04-26 07:58:05,139 - modelscope - WARNING - No preprocessor field found in cfg.\n",
            "2024-04-26 07:58:05,140 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
            "2024-04-26 07:58:05,144 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/root/.cache/modelscope/hub/damo/cv_resnet18_ocr-detection-line-level_damo'}. trying to build by task and model information.\n",
            "2024-04-26 07:58:05,145 - modelscope - WARNING - Find task: ocr-detection, model type: None. Insufficient information to build preprocessor, skip building preprocessor\n",
            "2024-04-26 07:58:05,149 - modelscope - INFO - loading model from dir /root/.cache/modelscope/hub/damo/cv_resnet18_ocr-detection-line-level_damo\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/modelscope/utils/device.py:60: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/modelscope/pipelines/cv/ocr_utils/ops.py:744: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "2024-04-26 07:58:07,590 - modelscope - INFO - loading model from /root/.cache/modelscope/hub/damo/cv_resnet18_ocr-detection-line-level_damo/tf_ckpts/checkpoint-80000\n",
            "2024-04-26 07:58:10,162 - modelscope - WARNING - Model revision not specified, use revision: v2.4.0\n",
            "Downloading: 100%|██████████| 1.51k/1.51k [00:00<00:00, 2.45MB/s]\n",
            "Downloading: 100%|██████████| 100k/100k [00:00<00:00, 2.11MB/s]\n",
            "Downloading: 100%|██████████| 73.3M/73.3M [00:00<00:00, 91.3MB/s]\n",
            "Downloading: 100%|██████████| 73.3M/73.3M [00:00<00:00, 110MB/s]\n",
            "Downloading: 100%|██████████| 5.14k/5.14k [00:00<00:00, 5.04MB/s]\n",
            "Downloading: 100%|██████████| 8.85k/8.85k [00:00<00:00, 8.42MB/s]\n",
            "Downloading: 100%|██████████| 52.0k/52.0k [00:00<00:00, 1.51MB/s]\n",
            "Downloading: 100%|██████████| 29.6k/29.6k [00:00<00:00, 2.16MB/s]\n",
            "2024-04-26 07:58:18,614 - modelscope - INFO - initiate model from /root/.cache/modelscope/hub/damo/cv_convnextTiny_ocr-recognition-document_damo\n",
            "2024-04-26 07:58:18,615 - modelscope - INFO - initiate model from location /root/.cache/modelscope/hub/damo/cv_convnextTiny_ocr-recognition-document_damo.\n",
            "2024-04-26 07:58:18,618 - modelscope - INFO - initialize model from /root/.cache/modelscope/hub/damo/cv_convnextTiny_ocr-recognition-document_damo\n",
            "2024-04-26 07:58:18,954 - modelscope - INFO - loading model from dir /root/.cache/modelscope/hub/damo/cv_convnextTiny_ocr-recognition-document_damo\n",
            "2024-04-26 07:58:18,995 - modelscope - INFO - loading model done\n"
          ]
        }
      ],
      "source": [
        "document_structurizer = DocumentStructurization(configs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wbGwLg3_tgj2"
      },
      "outputs": [],
      "source": [
        "def pdf2image(pdf_path):\n",
        "    \"\"\"\n",
        "    Convert a PDF file to an image.\n",
        "    \"\"\"\n",
        "    images = convert_from_path(pdf_path)\n",
        "\n",
        "    for i in range(len(images)):\n",
        "\n",
        "        images[i] = np.array(images[i])\n",
        "\n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F13oEt7mtgj2"
      },
      "outputs": [],
      "source": [
        "def extractFooter(image):\n",
        "    image = image[:2200,:, :]\n",
        "    fotter = image[2200:,:, :]\n",
        "    return image, fotter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UxnWV95dtgj2"
      },
      "outputs": [],
      "source": [
        "def merge_regions(results):\n",
        "    regions = []\n",
        "    for page in range(len(results) - 1):\n",
        "        if results[page][\"information\"][-1][\"category_name\"] == results[page+1][\"information\"][0][\"category_name\"]:\n",
        "            if results[page][\"information\"][-1][\"category_name\"] == \"table\":\n",
        "                regions.append({\n",
        "                    \"page\": page,\n",
        "                    \"region_poly1\": results[page][\"information\"][-1][\"region_poly\"],\n",
        "                    \"region_poly2\": results[page+1][\"information\"][0][\"region_poly\"],\n",
        "                })\n",
        "                results[page][\"information\"][-1] = {}\n",
        "                results[page+1][\"information\"][0] = {}\n",
        "\n",
        "    return results, regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ouU2rhYvtgj2"
      },
      "outputs": [],
      "source": [
        "def order_file(results):\n",
        "    results.sort(key=lambda x: (x[\"region_poly\"][1]))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OPfQCTPktgj2"
      },
      "outputs": [],
      "source": [
        "def structure1image(image):\n",
        "\n",
        "    final_result = document_structurizer(image)\n",
        "\n",
        "    final_result = order_file(final_result)\n",
        "\n",
        "    document_structurization_visualization(final_result, image)\n",
        "\n",
        "    return final_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5xwSxEYFtgj2"
      },
      "outputs": [],
      "source": [
        "def document_structure(path):\n",
        "    images = pdf2image(path)\n",
        "    images = [extractFooter(image)[0] for image in images]\n",
        "\n",
        "    doc = [\n",
        "        {\n",
        "            \"page\": 0,\n",
        "            \"content\": structure1image(images[0])\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    if len(images) > 1:\n",
        "        for i in range(1, len(images)):\n",
        "            tmp = structure1image(images[i])\n",
        "            if doc[i-1][\"content\"][-1][\"category_name\"] == tmp[0][\"category_name\"] and doc[i-1][\"content\"][-1][\"category_name\"] == \"table\":\n",
        "                img1 = images[i-1][doc[i-1][\"content\"][-1][\"region_poly\"][1]:doc[i-1][\"content\"][-1][\"region_poly\"][-1],:,:]\n",
        "                img2 = images[i][tmp[0][\"region_poly\"][1]:tmp[0][\"region_poly\"][-1],:,:]\n",
        "\n",
        "                concatenated_img = np.concatenate((img1, img2), axis=0)\n",
        "\n",
        "                plt.imsave(f\"tables/table-{random.randint(0,100)}-page{i}.jpg\", concatenated_img)\n",
        "\n",
        "                # plt.imshow(concatenated_img)\n",
        "\n",
        "                # result_merge = structure1image(concatenated_img)\n",
        "\n",
        "                doc[i-1][\"content\"] = doc[i-1][\"content\"][:-2]\n",
        "\n",
        "\n",
        "\n",
        "                doc.append(\n",
        "                    {\n",
        "                        \"page\": i,\n",
        "                        \"content\": structure1image(images[i])[1:]\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                doc.append({\n",
        "                    \"page\": i,\n",
        "                    \"content\": structure1image(images[i])\n",
        "                })\n",
        "\n",
        "    return(doc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7YI4mUD4tgj2"
      },
      "outputs": [],
      "source": [
        "output = document_structure(\"purchasing_contract_example.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "beMJxZ6H1htB"
      },
      "outputs": [],
      "source": [
        "def json2df(jsonData):\n",
        "  # try:\n",
        "    data = []\n",
        "    for i in range(len(jsonData)):\n",
        "      page = jsonData[i][\"content\"]\n",
        "      for j in range(len(page)):\n",
        "        region_poly = page[j][\"region_poly\"]\n",
        "        category_name = page[j][\"category_name\"]\n",
        "        content = \" \".join([page[j][\"text_list\"][k][\"content\"][0] for k in range(len(page[j][\"text_list\"]))])\n",
        "        page_number = i\n",
        "        data.append(\n",
        "            {\n",
        "                \"region\": region_poly,\n",
        "                \"category_name\": category_name,\n",
        "                \"content\": content,\n",
        "                \"page\": int(page_number)\n",
        "            }\n",
        "        )\n",
        "    return pd.DataFrame(data, columns = data[0].keys())\n",
        "  # except:\n",
        "  #   print(page[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "uxLhNznx2vAV"
      },
      "outputs": [],
      "source": [
        "df = json2df(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "N2K97xp_e9ft"
      },
      "outputs": [],
      "source": [
        "tables = df[df[\"category_name\"] == \"table\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "zUJ06B1j21tn"
      },
      "outputs": [],
      "source": [
        "def extractTables(pdf, df):\n",
        "  images = pdf2image(pdf)\n",
        "  tables = df[df[\"category_name\"] == \"table\"]\n",
        "  for i in range(tables.shape[0]):\n",
        "    region = tables.iloc[i][\"region\"]\n",
        "    page = int(tables.iloc[i][\"page\"])\n",
        "    img = images[page]\n",
        "    img = img[int(region[1])-30:int(region[-1])+30, :,:]\n",
        "    plt.imsave(f\"tables/table-{random.randint(0,100)}-page{tables.iloc[i]['page']}.jpg\", img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI51CpumQqL6"
      },
      "outputs": [],
      "source": [
        "extractTables(\"purchasing_contract_example.pdf\", df)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
=======
<<<<<<<< HEAD:docxchain.ipynb
      "execution_count": 1,
      "metadata": {
        "id": "sn_NW-txq81d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ipdb'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocxchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocxChain_PO\n\u001b[0;32m      3\u001b[0m docx_chain \u001b[38;5;241m=\u001b[39m DocxChain_PO()\n",
            "File \u001b[1;32mc:\\Users\\m.benkhadda\\docxchain\\docxchain.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_structurization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentStructurization\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_experimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SemanticChunker\n",
            "File \u001b[1;32mc:\\Users\\m.benkhadda\\docxchain\\pipelines\\document_structurization.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayout_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LayoutAnalysis\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextDetection\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_recognition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextRecognition\n",
            "File \u001b[1;32mc:\\Users\\m.benkhadda\\docxchain\\modules\\layout_analysis.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDocumentUnderstanding\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDocXLayout\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocXLayoutInfo, DocXLayoutPredictor\n\u001b[0;32m     11\u001b[0m BASE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLayoutAnalysis\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\m.benkhadda\\docxchain\\DocumentUnderstanding\\DocXLayout\\main.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuntie_subfield\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Huntie_Subfield\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetector_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detector_factory\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipdb\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipdb'"
          ]
        }
      ],
      "source": [
        "from docxchain import DocxChain_PO\n",
        "\n",
        "docx_chain = DocxChain_PO()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
========
>>>>>>> 0e8e17c8aa03e3dd124a6e71ba080edd34aad5af
      "execution_count": null,
      "metadata": {
        "id": "sn_NW-txq81d"
      },
<<<<<<< HEAD
=======
>>>>>>>> 0e8e17c8aa03e3dd124a6e71ba080edd34aad5af:docxchain/docxchain.ipynb
>>>>>>> 0e8e17c8aa03e3dd124a6e71ba080edd34aad5af
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
<<<<<<< HEAD
      "version": "3.11.8"
=======
<<<<<<<< HEAD:docxchain.ipynb
      "version": "3.9.19"
========
      "version": "3.11.8"
>>>>>>>> 0e8e17c8aa03e3dd124a6e71ba080edd34aad5af:docxchain/docxchain.ipynb
>>>>>>> 0e8e17c8aa03e3dd124a6e71ba080edd34aad5af
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
